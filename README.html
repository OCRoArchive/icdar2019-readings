<h1 id="readings-for-the-icdar2019-deep-learning-tutorial">Readings for the ICDAR2019 Deep Learning Tutorial</h1>
<h2 id="original-convolutional-networks">Original Convolutional Networks</h2>
<ul>
<li><a href="General/1995-lecun-convolutional.pdf">1995-lecun-convolutional</a>
<ul>
<li>convolutional networks, sigmoid, average pooling</li>
<li>precursor of RCNN for multi-object recognition</li>
<li>digits and handwriting</li>
</ul></li>
</ul>
<h2 id="convolutional-networks-on-gpus">Convolutional Networks on GPUs</h2>
<ul>
<li><a href="General/2013-krizhevsky-imagenet.pdf">2013-krizhevsky-imagenet</a>
<ul>
<li>ReLU, GPU training, local response normalization, pooling layers, dropout</li>
<li>Imagenet dataset</li>
</ul></li>
<li><a href="General/2014-srivastava-dropout.pdf">2014-srivastava-dropout</a>
<ul>
<li>dropouts as ensembles of networks</li>
<li>intended to prevent overtraining, improve generalization</li>
<li>standard test cases (CIFAR, MNIST, etc.)</li>
</ul></li>
<li><a href="General/2014-simonyan-maxpool-very-deep.pdf">2014-simonyan-maxpool-very-deep</a>
<ul>
<li>19 weight layers, multicrop evaluation, “VGG team” ILSVRC-2014 challenge</li>
</ul></li>
<li><a href="General/2015-ioffe-batch-normalization.pdf">2015-ioffe-batch-normalization</a>
<ul>
<li>introduces batch normalization for faster training</li>
</ul></li>
<li><a href="General/2015-szegedy-rethinking-inception.pdf">2015-szegedy-rethinking-inception</a>
<ul>
<li>label smoothing, separable convolutions</li>
</ul></li>
<li><a href="General/2015-szegedy-going-deeper.pdf">2015-szegedy-going-deeper</a>
<ul>
<li>“inception modules”, modular construction</li>
</ul></li>
<li><a href="General/2016-szegedy-inception.pdf">2016-szegedy-inception</a>
<ul>
<li>“inception modules”, modular construction</li>
</ul></li>
<li><a href="General/2015-he-resnet.pdf">2015-he-resnet</a>
<ul>
<li>Introduces Resnet architecture</li>
</ul></li>
<li><a href="General/2015-jaderberg-spatial-transformer.pdf">2015-jaderberg-spatial-transformer</a>
<ul>
<li>adds spatial transformations/distortions to learnable primitives</li>
</ul></li>
<li><a href="General/2017-dai-deformable.pdf">2017-dai-deformable</a>
<ul>
<li>adds deformable convolutions to learnable primitives</li>
</ul></li>
</ul>
<p>OCR:</p>
<ul>
<li><a href="OCR/2013-breuel-high-performance-ocr-lstm.pdf">2013-breuel-high-performance-ocr-lstm</a>
<ul>
<li>LSTM for printed OCR</li>
</ul></li>
<li><a href="OCR/2013-goodfellow-multidigit.pdf">2013-goodfellow-multidigit</a>
<ul>
<li>Google SVHN digits, 200k numbers with bounding boxes</li>
<li>8 layer convnet, ad-hoc sequence modeling</li>
</ul></li>
<li><a href="OCR/2017-breuel-lstm-ocr.pdf">2017-breuel-lstm-ocr</a>
<ul>
<li>comparison of different convnet+LSTM architectures for OCR</li>
</ul></li>
</ul>
<h2 id="segmentation-superresolution-with-convolutional-networks">Segmentation, Superresolution with Convolutional Networks</h2>
<ul>
<li><a href="General/2015-dong-superresolution.pdf">2015-dong-superresolution</a>
<ul>
<li>explicit upscaling of images</li>
</ul></li>
<li><a href="General/2015-ronneberger-unet.pdf">2015-ronneberger-unet</a>
<ul>
<li>general U-net architecture for image-to-image mappings</li>
</ul></li>
<li><a href="General/2015-byeon-mdlstm-segmentation.pdf">2015-byeon-mdlstm-segmentation</a>
<ul>
<li>MDLSTM for image segmentation</li>
</ul></li>
<li><a href="General/2015-stollenga-pyramid-lstm.pdf">2015-stollenga-pyramid-lstm</a>
<ul>
<li>pyramid LSTM architecture</li>
</ul></li>
<li><a href="General/2015-long-convnet-semantic-segmentation.pdf">2015-long-convnet-semantic-segmentation</a>
<ul>
<li>semantic segmentation with convolutional networks</li>
</ul></li>
<li><a href="General/2015-girshick-rich-feature-hierarchies.pdf">2015-girshick-rich-feature-hierarchies</a>
<ul>
<li>semantic segmentation with convolutional networks (multitask)</li>
</ul></li>
<li><a href="General/2015-noh-deconvolutional-networks.pdf">2015-noh-deconvolutional-networks</a>
<ul>
<li>atrous convolutions</li>
</ul></li>
<li><a href="Blogs/2017-blogpost-semantic-segmentation.pdf">2017-blogpost-semantic-segmentation</a>
<ul>
<li>survey of semantic segmentation architectures</li>
</ul></li>
<li><a href="General/2016-chen-deeplab.pdf">2016-chen-deeplab</a></li>
<li><a href="General/2017-chen-deeplab-atrous.pdf">2017-chen-deeplab-atrous</a></li>
<li><a href="General/2017-chen-rethinking-atrous.pdf">2017-chen-rethinking-atrous</a>
<ul>
<li>atrous convolutions to learnable primitives, deeplab v3</li>
</ul></li>
</ul>
<p>OCR:</p>
<ul>
<li><a href="OCR/2015-afzal-binarization-mdlstm.pdf">2015-afzal-binarization-mdlstm</a>
<ul>
<li>MDLSTM for binarization (image-to-image transformation)</li>
</ul></li>
<li><a href="OCR/2017-breuel-mdlstm-layout.pdf">2017-breuel-mdlstm-layout</a>
<ul>
<li>layout analysis with MDLSTM</li>
</ul></li>
<li><a href="OCR/2017-chen-convnet-page-segmentation.pdf">2017-chen-convnet-page-segmentation</a>
<ul>
<li>layout analysis with convolutional nteworks</li>
</ul></li>
<li><a href="OCR/2017-he-semantic-page-segmentation.pdf">2017-he-semantic-page-segmentation</a>
<ul>
<li>layout analysis with convolutional nteworks</li>
</ul></li>
<li><a href="OCR/2018-mohan-layout-error-correction-using-dnn.pdf">2018-mohan-layout-error-correction-using-dnn</a>
<ul>
<li>layout analysis with convolutional nteworks</li>
</ul></li>
</ul>
<!-- pix2pix etc. -->
<h2 id="rcnn-and-overfeat">RCNN and Overfeat</h2>
<ul>
<li><a href="General/2014-lecun-overfeat.pdf">2014-lecun-overfeat</a>
<ul>
<li>convolutional network, generic feature extraction</li>
<li>sliding window at multiple scales across image</li>
<li>regression network</li>
</ul></li>
<li><a href="General/2015-liu-multibox.pdf">2015-liu-multibox</a>
<ul>
<li>input image and ground truth boxes</li>
</ul></li>
<li><a href="General/2015-ren-faster-rcnn-v3.pdf">2015-ren-faster-rcnn-v3</a>
<ul>
<li>region proposal network (object/not object, box coords at each loc)</li>
<li>translation invariant anchors</li>
</ul></li>
</ul>
<p>OCR:</p>
<ul>
<li><a href="OCR/2014-jaderberg-convnet-ocr-wild.pdf">2014-jaderberg-convnet-ocr-wild</a>
<ul>
<li>convnet, R-CNN, bounding box regression</li>
<li>synthetic, ICDAR scene text, IIT Scene Text, IIT 5k words, IIT Sports-10k, BBC News</li>
<li>no bounding boxes in general; initial detector trained on positive word samples, negative images</li>
<li>10k proposals per image</li>
</ul></li>
</ul>
<h2 id="saliency-attention-visualization">Saliency, Attention, Visualization</h2>
<ul>
<li><a href="General/2014-jiang-saliency.pdf">2014-jiang-saliency</a>
<ul>
<li>explicit computation of salience</li>
</ul></li>
<li><a href="General/2015-zhou-class-attention-mapping.pdf">2015-zhou-class-attention-mapping</a>
<ul>
<li>gradient-based mapping of class-related features</li>
</ul></li>
<li><a href="General/2016-selvaraju-gradient-mapping.pdf">2016-selvaraju-gradient-mapping</a>
<ul>
<li>gradient-based mapping of class-related features</li>
</ul></li>
<li><a href="General/2013-zeiler-visualizing-cnns.pdf">2013-zeiler-visualizing-cnns</a>
<ul>
<li>learns inverses to layers via unpooling, transposed convolutions</li>
</ul></li>
<li><a href="General/2016-yu-visualizing-vgg.pdf">2016-yu-visualizing-vgg</a>
<ul>
<li>applied to VGG16</li>
</ul></li>
<li><a href="General/2018-li-pyramid-attention.pdf">2018-li-pyramid-attention</a>
<ul>
<li>combines multiresolution and attention</li>
</ul></li>
</ul>
<h2 id="lstm-ctc-gru">LSTM, CTC, GRU</h2>
<ul>
<li><a href="General/1999-gers-lstm.pdf">1999-gers-lstm</a>
<ul>
<li>introduces the LSTM architecture</li>
</ul></li>
<li><a href="General/2005-graves-bdlstm.pdf">2005-graves-bdlstm</a>
<ul>
<li>introduces bidirectional LSTM</li>
</ul></li>
<li><a href="General/2006-graves-ctc.pdf">2006-graves-ctc</a>
<ul>
<li>introduces CTC alignment (a kind of forward-backward algorithm)</li>
</ul></li>
</ul>
<p>OCR:</p>
<ul>
<li><a href="OCR/2012-elaguni-ocr-in-video.pdf">2012-elaguni-ocr-in-video</a>
<ul>
<li>manually labeled training data on small dataset</li>
<li>multiscale, convnet features, BLSTM, CTC</li>
</ul></li>
<li><a href="OCR/2014-bluche-comparison-sequence-trained.pdf">2014-bluche-comparison-sequence-trained</a>
<ul>
<li>HMM, GMM-HMM, MLP-HMM, LSTM</li>
<li>Rimes, IAM; decoding with Kaldi (ASR toolkit)</li>
</ul></li>
<li><a href="OCR/2016-he-reading-scene-text.pdf">2016-he-reading-scene-text</a>
<ul>
<li>large CNN, Maxout units, LSTM, CTC</li>
<li>Street View Text, IIT 5k-word, PhotoOCR, etc., using bounding boxes for training</li>
</ul></li>
<li><a href="OCR/2017-wang-gru-ocr.pdf">2017-wang-gru-ocr</a></li>
</ul>
<h2 id="d-lstm">2D LSTM</h2>
<ul>
<li><a href="General/2009-graves-multidimensional.pdf">2009-graves-multidimensional</a>
<ul>
<li>applies LSTM to multidimensional problems</li>
</ul></li>
<li><a href="General/2014-byeon-supervised-texture.pdf">2014-byeon-supervised-texture</a>
<ul>
<li>supervised image segmentation using multidimensional LSTM</li>
</ul></li>
<li><a href="General/2016-visin-reseg.pdf">2016-visin-reseg</a>
<ul>
<li>separable multidimensional LSTMs for image segmentation</li>
</ul></li>
<li><a href="General/2015-sonderby-convolutional.pdf">2015-sonderby-convolutional</a>
<ul>
<li>convolutional LSTM architecture and attention</li>
</ul></li>
<li><a href="General/2016-shi-convolutional-lstm.pdf">2016-shi-convolutional-lstm</a>
<ul>
<li>convolutional LSTM architecture</li>
</ul></li>
</ul>
<p>OCR:</p>
<ul>
<li><a href="OCR/2015-visin-renet.pdf">2015-visin-renet</a>
<ul>
<li>separable multidimensional LSTMs for OCR</li>
</ul></li>
</ul>
<h2 id="seq2seq-attention">Seq2Seq, Attention</h2>
<ul>
<li><a href="General/2012-graves-sequence-transduction.pdf">2012-graves-sequence-transduction</a>
<ul>
<li>introduces sequence transduction as an alternative to CTC</li>
</ul></li>
<li><a href="General/2015-bahdanau-attention.pdf">2015-bahdanau-attention</a>
<ul>
<li>content-based attention mechanisms for sequence to sequence tasks</li>
</ul></li>
<li><a href="General/2015-zhang-character-level-convnets-text.pdf">2015-zhang-character-level-convnets-text</a>
<ul>
<li>simple use of convolutional networks as alternatives to n-grams, sequence models</li>
</ul></li>
<li><a href="General/2016-chorowski-better-decoding.pdf">2016-chorowski-better-decoding</a>
<ul>
<li>label smoothing and beam search</li>
</ul></li>
<li><a href="General/2017-vaswani-attention-is-all-you-need.pdf">2017-vaswani-attention-is-all-you-need</a>
<ul>
<li>high performance sequence-to-sequence with attention</li>
<li>masked, multi-head attention</li>
</ul></li>
<li><a href="General/2017-prabhavalkar-s2s-comparison.pdf">2017-prabhavalkar-s2s-comparison</a>
<ul>
<li>a comparison of different sequence-to-sequence approaches</li>
</ul></li>
<li><a href="General/2017-gehring-convolutional-s2s.pdf">2017-gehring-convolutional-s2s</a>
<ul>
<li>purely convolutional sequence-to-sequence with attention</li>
</ul></li>
</ul>
<p>OCR:</p>
<ul>
<li><a href="OCR/2015-sahu-s2s-ocr.pdf">2015-sahu-s2s-ocr</a>
<ul>
<li>standard seq2seq encoder/decoder approach</li>
<li>TSNE visualizations of encoded word images</li>
<li>word images from scanned books</li>
</ul></li>
</ul>
<h2 id="visual-attention">Visual Attention</h2>
<ul>
<li><a href="General/2017-nam-dual-attention.pdf">2017-nam-dual-attention</a>
<ul>
<li>joint visual and text attention networks</li>
</ul></li>
</ul>
<p>OCR:</p>
<ul>
<li><a href="OCR/2016-bluche-end-to-end-hw-mdlstm-attention.pdf">2016-bluche-end-to-end-hw-mdlstm-attention</a>
<ul>
<li>full paragraph handwriting recognition without explicit segmentation</li>
<li>MDLSTM plus attention, tracking, etc.</li>
<li>IAM database, pretraining LSTM+CTC, curriculum learning</li>
</ul></li>
<li><a href="OCR/2016-lee-recursive-recurrent-attention-wild.pdf">2016-lee-recursive-recurrent-attention-wild</a>
<ul>
<li>recursive convolutional layers, tied weights, followed by attention, character level modeling</li>
<li>ICDAR 2003, 2013, SVT, IIT5k, Synth90k using bounding boxes for training</li>
</ul></li>
</ul>
<!-- 

## Language Modeling

- [2016-rosca-lstm-transcript](OCR/2016-rosca-lstm-transcript.pdf)

## Domain Adaptation, Unsupervised, Semi-Supervised, Multitask Learning

Domain Adaptation:

- [2017-liu-unsupervised-domain-adaptation](Learning/2017-liu-unsupervised-domain-adaptation.pdf)
- [2017-tzen-adversarial-domain-discriminator-adaptation](Learning/2017-tzen-adversarial-domain-discriminator-adaptation.pdf)

Semi-Supervised Learning:

- [2005-zhu-semi-supervised](Learning/2005-zhu-semi-supervised.pdf)
    - classical methods of semi-supervised learning
- [2017-li-noisy-labels-distillation](Learning/2017-li-noisy-labels-distillation.pdf)
    - uses distillation for dealing with noisy lables
- [2018-oliver-evaluation-semi-supervised](Learning/2018-oliver-evaluation-semi-supervised.pdf)

- [2018-ren-metalearning-semi-supervised](Learning/2018-ren-metalearning-semi-supervised.pdf)
- [2018-tanaka-joint-optimization-noisy-labels](Learning/2018-tanaka-joint-optimization-noisy-labels.pdf)

Examples of Unsupervised Learning:

- [2016-lin-unsupervised-binary-descriptors](Learning/2016-lin-unsupervised-binary-descriptors.pdf)
- [2016-radford-unsupervised-representation-learning](Learning/2016-radford-unsupervised-representation-learning.pdf)
- [2016-xie-unsupervised-deep-embedding](Learning/2016-xie-unsupervised-deep-embedding.pdf)
- [2017-ren-unsupervised-deep-flow](Learning/2017-ren-unsupervised-deep-flow.pdf)
- [2017-lotter-unsupervised-predictive-video-coding](Learning/2017-lotter-unsupervised-predictive-video-coding.pdf)
- [2018-li-unsupervised-odometry](Learning/2018-li-unsupervised-odometry.pdf)

Transfer and Multitask Learning:

- [2016-geng-transfer-learning-reid](Learning/2016-geng-transfer-learning-reid.pdf)
- [2016-rusu-progressive-networks](Learning/2016-rusu-progressive-networks.pdf)
- [2017-ruder-multitask-survey](Learning/2017-ruder-multitask-survey.pdf)

## GANs

- [2014-goodfellow-gans](General/2014-goodfellow-gans.pdf)
- [2015-radford-dcgan](General/2015-radford-dcgan.pdf)
- [2016-isola-image2image-gan](General/2016-isola-image2image-gan.pdf)
- [2016-salimans-improved-gan-training](General/2016-salimans-improved-gan-training.pdf)

- [2016-ho-gan-imitation-learning](General/2016-ho-gan-imitation-learning.pdf)
## Siamese

## Computational Issues

- [2017-chen-distributed-sgd](General/2017-chen-distributed-sgd.pdf)
    - scaling up with many GPUs on many nodes
- [2016-iandola-squeezenet](General/2016-iandola-squeezenet.pdf)
    - reducing the number of parameters
- [2017-yuan-adversarial](General/2017-yuan-adversarial.pdf)
    - the existence and problems with adversarial samples

# Surveys

- [2014-schmidhuber-deep-learning-survey](General/2014-schmidhuber-deep-learning-survey.pdf)
- [2015-lecun-nature-deep-learning](General/2015-lecun-nature-deep-learning.pdf)
- [2015-karpathy-recurrent-ocr](Blogs/2015-karpathy-recurrent-ocr.pdf)
- [2018-alom-survey-imagenet](General/2018-alom-survey-imagenet.pdf)

# Additional Readings

- [2013-goodfellow-maxout](More/2013-goodfellow-maxout.pdf)
- [2014-donahue-long-term-rcnn](General/2014-donahue-long-term-rcnn.pdf)
- [2014-karpathy-image-descriptions](General/2014-karpathy-image-descriptions.pdf)
- [2015-liu-face-attributes-wild](General/2015-liu-face-attributes-wild.pdf)
- [2015-mnih-deep-reinforcement-learning](General/2015-mnih-deep-reinforcement-learning.pdf)
- [2015-ng-video-classification](General/2015-ng-video-classification.pdf)
- [2015-yu-visual-madlibs](General/2015-yu-visual-madlibs.pdf)
- [2015-zheng-crfs-as-rnns](General/2015-zheng-crfs-as-rnns.pdf)
- [2016-abadi-tensorflow](General/2016-abadi-tensorflow.pdf)
- [2016-ba-layernorm](More/2016-ba-layernorm.pdf)
- [2016-mnih-async-dl](General/2016-mnih-async-dl.pdf)
- [2016-salimans-weightnorm](More/2016-salimans-weightnorm.pdf)
- [2016-shi-superresolution](More/2016-shi-superresolution.pdf)
- [2016-ulyanof-instancenorm](More/2016-ulyanof-instancenorm.pdf)
- [2016-vinyals-matching-networks](General/2016-vinyals-matching-networks.pdf)
- [2016-zhang-very-deep-speech](General/2016-zhang-very-deep-speech.pdf)
- [2017-barron-celu](More/2017-barron-celu.pdf)
- [2017-hochrreiter-self-normalizing-networks](More/2017-hochrreiter-self-normalizing-networks.pdf)
- [2017-ioffe-batchnorm](More/2017-ioffe-batchnorm.pdf)
- [2017-wang-tacotron](General/2017-wang-tacotron.pdf)
- [2018-burda-curiosity](General/2018-burda-curiosity.pdf)
- [2018-metz-metalearning](General/2018-metz-metalearning.pdf)
- [2018-wu-groupnorm](More/2018-wu-groupnorm.pdf)

-->
