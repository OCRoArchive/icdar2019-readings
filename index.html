<h1 id="readings-for-the-icdar2019-deep-learning-tutorial">Readings for the ICDAR2019 Deep Learning Tutorial</h1>
<h2 id="original-convolutional-networks">Original Convolutional Networks</h2>
<ul>
<li><a href="General/1995-lecun-convolutional.pdf">1995-lecun-convolutional</a></li>
</ul>
<h2 id="convolutional-networks-on-gpus">Convolutional Networks on GPUs</h2>
<ul>
<li><a href="General/2013-krizhevsky-imagenet.pdf">2013-krizhevsky-imagenet</a></li>
</ul>
<h2 id="rcnn">RCNN</h2>
<ul>
<li><a href="General/2013-girshick-rcnn.pdf">2013-girshick-rcnn</a></li>
<li><a href="General/2015-girshick-fast-rcnn.pdf">2015-girshick-fast-rcnn</a></li>
<li><a href="General/2015-ren-faster-rcnn.pdf">2015-ren-faster-rcnn</a></li>
<li><a href="General/2015-ren-faster-rcnn-v3.pdf">2015-ren-faster-rcnn-v3</a></li>
</ul>
<h2 id="visualizing">Visualizing</h2>
<ul>
<li><a href="General/2013-zeiler-visualizing-cnns.pdf">2013-zeiler-visualizing-cnns</a></li>
<li><a href="General/2016-yu-visualizing-vgg.pdf">2016-yu-visualizing-vgg</a></li>
</ul>
<h2 id="relu-maxpool-dropout">ReLU, MaxPool, Dropout</h2>
<ul>
<li><a href="General/2014-srivastava-dropout.pdf">2014-srivastava-dropout</a></li>
<li><a href="General/2014-lecun-overfeat.pdf">2014-lecun-overfeat</a></li>
<li><a href="General/2014-simonyan-maxpool-very-deep.pdf">2014-simonyan-maxpool-very-deep</a></li>
<li><a href="General/2015-szegedy-going-deeper.pdf">2015-szegedy-going-deeper</a></li>
</ul>
<h2 id="superresolution">Superresolution</h2>
<ul>
<li><a href="General/2015-dong-superresolution.pdf">2015-dong-superresolution</a></li>
</ul>
<h2 id="gans">GANs</h2>
<ul>
<li><a href="General/2014-goodfellow-gans.pdf">2014-goodfellow-gans</a></li>
<li><a href="General/2015-radford-dcgan.pdf">2015-radford-dcgan</a></li>
</ul>
<!-- # Siamese -->
<h2 id="saliency">Saliency</h2>
<ul>
<li><a href="General/2014-jiang-saliency.pdf">2014-jiang-saliency</a></li>
</ul>
<h2 id="batchnorm">Batchnorm</h2>
<ul>
<li><a href="General/2015-ioffe-batch-normalization.pdf">2015-ioffe-batch-normalization</a></li>
</ul>
<h2 id="unet">UNET</h2>
<ul>
<li><a href="General/2015-ronnenberger-unet.pdf">2015-ronnenberger-unet</a></li>
</ul>
<h2 id="resnet">Resnet</h2>
<ul>
<li><a href="General/2015-he-resnet.pdf">2015-he-resnet</a></li>
<li><a href="General/2016-szegedy-inception.pdf">2016-szegedy-inception</a></li>
</ul>
<h2 id="strided-and-atrous-convolutions">Strided and Atrous Convolutions</h2>
<ul>
<li><a href="General/2017-chen-deeplab-atrous.pdf">2017-chen-deeplab-atrous</a></li>
<li><a href="General/2017-chen-rethinking-atrous.pdf">2017-chen-rethinking-atrous</a></li>
</ul>
<h2 id="transformer-networks">Transformer Networks</h2>
<ul>
<li><a href="General/2015-jaderberg-spatial-transformer.pdf">2015-jaderberg-spatial-transformer</a></li>
</ul>
<h2 id="lstm-ctc-gru">LSTM, CTC, GRU</h2>
<ul>
<li><a href="General/1999-gers-lstm.pdf">1999-gers-lstm</a></li>
<li><a href="General/2005-graves-bdlstm.pdf">2005-graves-bdlstm</a></li>
<li><a href="General/2006-graves-ctc.pdf">2006-graves-ctc</a></li>
</ul>
<h2 id="d-lstm">2D LSTM</h2>
<ul>
<li><a href="General/2009-graves-multidimensional.pdf">2009-graves-multidimensional</a></li>
<li><a href="General/2014-byeon-supervised-texture.pdf">2014-byeon-supervised-texture</a></li>
<li><a href="General/2016-visin-reseg.pdf">2016-visin-reseg</a></li>
<li><a href="OCR/2015-visin-renet.pdf">2015-visin-renet</a></li>
</ul>
<h2 id="seq2seq-attention">Seq2Seq, Attention</h2>
<ul>
<li><a href="General/2012-graves-sequence-transduction.pdf">2012-graves-sequence-transduction</a></li>
<li><a href="General/2016-chorowski-better-decoding.pdf">2016-chorowski-better-decoding</a></li>
<li><a href="General/2015-bahdanau-attention.pdf">2015-bahdanau-attention</a></li>
<li><a href="General/2017-vaswani-attention-is-all-you-need.pdf">2017-vaswani-attention-is-all-you-need</a></li>
<li><a href="General/2017-prabhavalkar-s2s-comparison.pdf">2017-prabhavalkar-s2s-comparison</a></li>
</ul>
<h2 id="visual-attention">Visual Attention</h2>
<ul>
<li><a href="General/2017-nam-dual-attention.pdf">2017-nam-dual-attention</a></li>
<li><a href="General/2016-you-dual-attention.pdf">2016-you-dual-attention</a></li>
</ul>
<h2 id="deformable-convolutions">Deformable Convolutions</h2>
<ul>
<li><a href="General/2017-dai-deformable.pdf">2017-dai-deformable</a></li>
</ul>
<h2 id="parallel-and-distributed-training">Parallel and Distributed Training</h2>
<ul>
<li><a href="General/2017-chen-distributed-sgd.pdf">2017-chen-distributed-sgd</a></li>
</ul>
<h2 id="adversarial-samples">Adversarial Samples</h2>
<ul>
<li><a href="General/2017-yuan-adversarial.pdf">2017-yuan-adversarial</a></li>
</ul>
<h2 id="squeezing">Squeezing</h2>
<ul>
<li><a href="General/2016-iandola-squeezenet.pdf">2016-iandola-squeezenet</a></li>
</ul>
<h2 id="surveys">Surveys</h2>
<ul>
<li><a href="General/2014-schmidhuber-deep-learning-survey.pdf">2014-schmidhuber-deep-learning-survey</a></li>
<li><a href="General/2015-lecun-nature-deep-learning.pdf">2015-lecun-nature-deep-learning</a></li>
<li><a href="OCR/2015-karpathy-recurrent-ocr.pdf">2015-karpathy-recurrent-ocr</a></li>
<li><a href="General/2018-alom-survey-imagenet.pdf">2018-alom-survey-imagenet</a></li>
</ul>
<h2 id="ocr">OCR</h2>
<ul>
<li><a href="OCR/2012-elaguni-ocr-in-video.pdf">2012-elaguni-ocr-in-video</a>
<ul>
<li>manually labeled training data on small dataset</li>
<li>multiscale, convnet features, BLSTM, CTC</li>
</ul></li>
<li><a href="OCR/2013-goodfellow-multidigit.pdf">2013-goodfellow-multidigit</a>
<ul>
<li>Google SVHN digits, 200k numbers with bounding boxes</li>
<li>8 layer convnet, ad-hoc sequence modeling</li>
</ul></li>
<li><a href="OCR/2014-bluche-comparison-sequence-trained.pdf">2014-bluche-comparison-sequence-trained</a>
<ul>
<li>HMM, GMM-HMM, MLP-HMM, LSTM</li>
<li>Rimes, IAM; decoding with Kaldi (ASR toolkit)</li>
</ul></li>
<li><a href="OCR/2014-jaderberg-convnet-ocr-wild.pdf">2014-jaderberg-convnet-ocr-wild</a>
<ul>
<li>convnet, R-CNN, bounding box regression</li>
<li>synthetic, ICDAR scene text, IIT Scene Text, IIT 5k words, IIT Sports-10k, BBC News</li>
<li>no bounding boxes in general; initial detector trained on positive word samples, negative images</li>
<li>10k proposals per image</li>
</ul></li>
<li><a href="OCR/2015-sahu-s2s-ocr.pdf">2015-sahu-s2s-ocr</a>
<ul>
<li>standard seq2seq encoder/decoder approach</li>
<li>TSNE visualizations of encoded word images</li>
<li>word images from scanned books</li>
</ul></li>
<li><a href="OCR/2016-bluche-end-to-end-hw-mdlstm-attention.pdf">2016-bluche-end-to-end-hw-mdlstm-attention</a>
<ul>
<li>full paragraph handwriting recognition without explicit segmentation</li>
<li>MDLSTM plus attention, tracking, etc.</li>
<li>IAM database, pretraining LSTM+CTC, curriculum learning</li>
</ul></li>
<li><a href="OCR/2016-he-reading-scene-text.pdf">2016-he-reading-scene-text</a>
<ul>
<li>large CNN, Maxout units, LSTM, CTC</li>
<li>Street View Text, IIT 5k-word, PhotoOCR, etc., using bounding boxes for training</li>
</ul></li>
<li><a href="OCR/2016-lee-recursive-recurrent-attention-wild.pdf">2016-lee-recursive-recurrent-attention-wild</a>
<ul>
<li>recursive convolutional layers, tied weights, followed by attention, character level modeling</li>
<li>ICDAR 2003, 2013, SVT, IIT5k, Synth90k using bounding boxes for training</li>
</ul></li>
</ul>
<h2 id="additional-readings">Additional Readings</h2>
<ul>
<li><a href="General/2014-donahue-long-term-rcnn.pdf">2014-donahue-long-term-rcnn</a></li>
<li><a href="General/2014-karpathy-image-descriptions.pdf">2014-karpathy-image-descriptions</a></li>
<li><a href="General/2015-liu-face-attributes-wild.pdf">2015-liu-face-attributes-wild</a></li>
<li><a href="General/2015-mnih-deep-reinforcement-learning.pdf">2015-mnih-deep-reinforcement-learning</a></li>
<li><a href="General/2015-ng-video-classification.pdf">2015-ng-video-classification</a></li>
<li><a href="General/2015-ronneberger-unet.pdf">2015-ronneberger-unet</a></li>
<li><a href="General/2015-yu-visual-madlibs.pdf">2015-yu-visual-madlibs</a></li>
<li><a href="General/2015-zhang-character-level-convnets-text.pdf">2015-zhang-character-level-convnets-text</a></li>
<li><a href="General/2015-zheng-crfs-as-rnns.pdf">2015-zheng-crfs-as-rnns</a></li>
<li><a href="General/2016-abadi-tensorflow.pdf">2016-abadi-tensorflow</a></li>
<li><a href="General/2016-mnih-async-dl.pdf">2016-mnih-async-dl</a></li>
<li><a href="General/2018-burda-curiosity.pdf">2018-burda-curiosity</a></li>
<li><a href="General/2018-metz-metalearning.pdf">2018-metz-metalearning</a></li>
<li><a href="General/2017-wang-tacotron.pdf">2017-wang-tacotron</a></li>
<li><a href="General/2016-vinyals-matching-networks.pdf">2016-vinyals-matching-networks</a></li>
<li><a href="General/2016-zhang-very-deep-speech.pdf">2016-zhang-very-deep-speech</a></li>
</ul>
