<h1 id="readings-for-the-icdar2019-deep-learning-tutorial">Readings for the ICDAR2019 Deep Learning Tutorial</h1>
<h2 id="original-convolutional-networks">Original Convolutional Networks</h2>
<ul>
<li><a href="General/1995-lecun-convolutional.pdf">1995-lecun-convolutional</a>
<ul>
<li>convolutional networks, sigmoid, average pooling</li>
<li>precursor of RCNN for multi-object recognition</li>
<li>digits and handwriting</li>
</ul></li>
</ul>
<h2 id="convolutional-networks-on-gpus">Convolutional Networks on GPUs</h2>
<ul>
<li><a href="General/2013-krizhevsky-imagenet.pdf">2013-krizhevsky-imagenet</a>
<ul>
<li>ReLU, GPU training, local response normalization, pooling layers, dropout</li>
<li>Imagenet dataset</li>
</ul></li>
<li><a href="General/2014-srivastava-dropout.pdf">2014-srivastava-dropout</a>
<ul>
<li>dropouts as ensembles of networks</li>
<li>intended to prevent overtraining, improve generalization</li>
<li>standard test cases (CIFAR, MNIST, etc.)</li>
</ul></li>
<li><a href="General/2014-simonyan-maxpool-very-deep.pdf">2014-simonyan-maxpool-very-deep</a>
<ul>
<li>19 weight layers, multicrop evaluation, “VGG team” ILSVRC-2014 challenge</li>
</ul></li>
<li><a href="General/2015-ioffe-batch-normalization.pdf">2015-ioffe-batch-normalization</a>
<ul>
<li>introduces batch normalization for faster training</li>
</ul></li>
<li><a href="General/2015-szegedy-rethinking-inception.pdf">2015-szegedy-rethinking-inception</a>
<ul>
<li>label smoothing, separable convolutions</li>
</ul></li>
<li><a href="General/2015-szegedy-going-deeper.pdf">2015-szegedy-going-deeper</a>
<ul>
<li>“inception modules”, modular construction</li>
</ul></li>
<li><a href="General/2016-szegedy-inception.pdf">2016-szegedy-inception</a>
<ul>
<li>“inception modules”, modular construction</li>
</ul></li>
<li><a href="General/2015-he-resnet.pdf">2015-he-resnet</a>
<ul>
<li>Introduces Resnet architecture</li>
</ul></li>
</ul>
<p>OCR:</p>
<ul>
<li><a href="OCR/2013-goodfellow-multidigit.pdf">2013-goodfellow-multidigit</a>
<ul>
<li>Google SVHN digits, 200k numbers with bounding boxes</li>
<li>8 layer convnet, ad-hoc sequence modeling</li>
</ul></li>
</ul>
<h2 id="segmentation-superresolution-with-convolutional-networks">Segmentation, Superresolution with Convolutional Networks</h2>
<ul>
<li><a href="General/2015-dong-superresolution.pdf">2015-dong-superresolution</a>
<ul>
<li>explicit upscaling of images</li>
</ul></li>
<li><a href="General/2015-ronneberger-unet.pdf">2015-ronneberger-unet</a>
<ul>
<li>general U-net architecture for image-to-image mappings</li>
</ul></li>
</ul>
<!-- more pixel2pixel, image2image transformations -->
<h2 id="additional-kernels">Additional Kernels</h2>
<ul>
<li><a href="General/2015-jaderberg-spatial-transformer.pdf">2015-jaderberg-spatial-transformer</a>
<ul>
<li>adds spatial transformations/distortions to learnable primitives</li>
</ul></li>
<li><a href="General/2017-dai-deformable.pdf">2017-dai-deformable</a>
<ul>
<li>adds deformable convolutions to learnable primitives</li>
</ul></li>
<li><a href="General/2017-chen-deeplab-atrous.pdf">2017-chen-deeplab-atrous</a>
<ul>
<li>adds atrous convolutions to learnable primitives</li>
</ul></li>
<li><a href="General/2017-chen-rethinking-atrous.pdf">2017-chen-rethinking-atrous</a>
<ul>
<li>adds atrous convolutions to learnable primitives</li>
</ul></li>
</ul>
<h2 id="rcnn-and-overfeat">RCNN and Overfeat</h2>
<ul>
<li><a href="General/2014-lecun-overfeat.pdf">2014-lecun-overfeat</a>
<ul>
<li>convolutional network, generic feature extraction</li>
<li>sliding window at multiple scales across image</li>
<li>regression network</li>
</ul></li>
<li><a href="General/2015-liu-multibox.pdf">2015-liu-multibox</a>
<ul>
<li>input image and ground truth boxes</li>
</ul></li>
<li><a href="General/2015-ren-faster-rcnn-v3.pdf">2015-ren-faster-rcnn-v3</a>
<ul>
<li>region proposal network (object/not object, box coords at each loc)</li>
<li>translation invariant anchors</li>
</ul></li>
</ul>
<p>OCR:</p>
<ul>
<li><a href="OCR/2014-jaderberg-convnet-ocr-wild.pdf">2014-jaderberg-convnet-ocr-wild</a>
<ul>
<li>convnet, R-CNN, bounding box regression</li>
<li>synthetic, ICDAR scene text, IIT Scene Text, IIT 5k words, IIT Sports-10k, BBC News</li>
<li>no bounding boxes in general; initial detector trained on positive word samples, negative images</li>
<li>10k proposals per image</li>
</ul></li>
</ul>
<h2 id="saliency-attention-visualization">Saliency, Attention, Visualization</h2>
<ul>
<li><a href="General/2014-jiang-saliency.pdf">2014-jiang-saliency</a>
<ul>
<li>explicit computation of salience</li>
</ul></li>
<li><a href="General/2015-zhou-class-attention-mapping.pdf">2015-zhou-class-attention-mapping</a>
<ul>
<li>gradient-based mapping of class-related features</li>
</ul></li>
<li><a href="General/2016-selvaraju-gradient-mapping.pdf">2016-selvaraju-gradient-mapping</a>
<ul>
<li>gradient-based mapping of class-related features</li>
</ul></li>
<li><a href="General/2013-zeiler-visualizing-cnns.pdf">2013-zeiler-visualizing-cnns</a>
<ul>
<li>learns inverses to layers via unpooling, transposed convolutions</li>
</ul></li>
<li><a href="General/2016-yu-visualizing-vgg.pdf">2016-yu-visualizing-vgg</a>
<ul>
<li>applied to VGG16</li>
</ul></li>
</ul>
<h2 id="lstm-ctc-gru">LSTM, CTC, GRU</h2>
<ul>
<li><a href="General/1999-gers-lstm.pdf">1999-gers-lstm</a>
<ul>
<li>introduces the LSTM architecture</li>
</ul></li>
<li><a href="General/2005-graves-bdlstm.pdf">2005-graves-bdlstm</a>
<ul>
<li>introduces bidirectional LSTM</li>
</ul></li>
<li><a href="General/2006-graves-ctc.pdf">2006-graves-ctc</a>
<ul>
<li>introduces CTC alignment (a kind of forward-backward algorithm)</li>
</ul></li>
</ul>
<p>OCR:</p>
<ul>
<li><a href="OCR/2012-elaguni-ocr-in-video.pdf">2012-elaguni-ocr-in-video</a>
<ul>
<li>manually labeled training data on small dataset</li>
<li>multiscale, convnet features, BLSTM, CTC</li>
</ul></li>
<li><a href="OCR/2014-bluche-comparison-sequence-trained.pdf">2014-bluche-comparison-sequence-trained</a>
<ul>
<li>HMM, GMM-HMM, MLP-HMM, LSTM</li>
<li>Rimes, IAM; decoding with Kaldi (ASR toolkit)</li>
</ul></li>
<li><a href="OCR/2016-he-reading-scene-text.pdf">2016-he-reading-scene-text</a>
<ul>
<li>large CNN, Maxout units, LSTM, CTC</li>
<li>Street View Text, IIT 5k-word, PhotoOCR, etc., using bounding boxes for training</li>
</ul></li>
</ul>
<h2 id="d-lstm">2D LSTM</h2>
<ul>
<li><a href="General/2009-graves-multidimensional.pdf">2009-graves-multidimensional</a>
<ul>
<li>applies LSTM to multidimensional problems</li>
</ul></li>
<li><a href="General/2014-byeon-supervised-texture.pdf">2014-byeon-supervised-texture</a>
<ul>
<li>supervised image segmentation using multidimensional LSTM</li>
</ul></li>
<li><a href="General/2016-visin-reseg.pdf">2016-visin-reseg</a>
<ul>
<li>separable multidimensional LSTMs for image segmentation</li>
</ul></li>
<li><a href="General/2015-sonderby-convolutional.pdf">2015-sonderby-convolutional</a>
<ul>
<li>convolutional LSTM architecture and attention</li>
</ul></li>
<li><a href="General/2016-shi-convolutional-lstm.pdf">2016-shi-convolutional-lstm</a>
<ul>
<li>convolutional LSTM architecture</li>
</ul></li>
</ul>
<p>OCR:</p>
<ul>
<li><a href="OCR/2015-visin-renet.pdf">2015-visin-renet</a>
<ul>
<li>separable multidimensional LSTMs for OCR</li>
</ul></li>
</ul>
<h2 id="seq2seq-attention">Seq2Seq, Attention</h2>
<ul>
<li><a href="General/2012-graves-sequence-transduction.pdf">2012-graves-sequence-transduction</a>
<ul>
<li>introduces sequence transduction as an alternative to CTC</li>
</ul></li>
<li><a href="General/2015-bahdanau-attention.pdf">2015-bahdanau-attention</a>
<ul>
<li>content-based attention mechanisms for sequence to sequence tasks</li>
</ul></li>
<li><a href="General/2015-zhang-character-level-convnets-text.pdf">2015-zhang-character-level-convnets-text</a>
<ul>
<li>simple use of convolutional networks as alternatives to n-grams, sequence models</li>
</ul></li>
<li><a href="General/2016-chorowski-better-decoding.pdf">2016-chorowski-better-decoding</a>
<ul>
<li>label smoothing and beam search</li>
</ul></li>
<li><a href="General/2017-vaswani-attention-is-all-you-need.pdf">2017-vaswani-attention-is-all-you-need</a>
<ul>
<li>high performance sequence-to-sequence with attention</li>
<li>masked, multi-head attention</li>
</ul></li>
<li><a href="General/2017-prabhavalkar-s2s-comparison.pdf">2017-prabhavalkar-s2s-comparison</a>
<ul>
<li>a comparison of different sequence-to-sequence approaches</li>
</ul></li>
<li><a href="General/2017-gehring-convolutional-s2s.pdf">2017-gehring-convolutional-s2s</a>
<ul>
<li>purely convolutional sequence-to-sequence with attention</li>
</ul></li>
</ul>
<p>OCR:</p>
<ul>
<li><a href="OCR/2015-sahu-s2s-ocr.pdf">2015-sahu-s2s-ocr</a>
<ul>
<li>standard seq2seq encoder/decoder approach</li>
<li>TSNE visualizations of encoded word images</li>
<li>word images from scanned books</li>
</ul></li>
</ul>
<h2 id="visual-attention">Visual Attention</h2>
<ul>
<li><a href="General/2017-nam-dual-attention.pdf">2017-nam-dual-attention</a>
<ul>
<li>joint visual and text attention networks</li>
</ul></li>
</ul>
<p>OCR:</p>
<ul>
<li><a href="OCR/2016-bluche-end-to-end-hw-mdlstm-attention.pdf">2016-bluche-end-to-end-hw-mdlstm-attention</a>
<ul>
<li>full paragraph handwriting recognition without explicit segmentation</li>
<li>MDLSTM plus attention, tracking, etc.</li>
<li>IAM database, pretraining LSTM+CTC, curriculum learning</li>
</ul></li>
<li><a href="OCR/2016-lee-recursive-recurrent-attention-wild.pdf">2016-lee-recursive-recurrent-attention-wild</a>
<ul>
<li>recursive convolutional layers, tied weights, followed by attention, character level modeling</li>
<li>ICDAR 2003, 2013, SVT, IIT5k, Synth90k using bounding boxes for training</li>
</ul></li>
</ul>
<h2 id="domain-adaptation-unsupervised-semi-supervised-multitask-learning">Domain Adaptation, Unsupervised, Semi-Supervised, Multitask Learning</h2>
<p>Domain Adaptation:</p>
<ul>
<li><a href="Learning/2017-liu-unsupervised-domain-adaptation.pdf">2017-liu-unsupervised-domain-adaptation</a></li>
<li><a href="Learning/2017-tzen-adversarial-domain-discriminator-adaptation.pdf">2017-tzen-adversarial-domain-discriminator-adaptation</a></li>
</ul>
<p>Semi-Supervised Learning:</p>
<ul>
<li><a href="Learning/2005-zhu-semi-supervised.pdf">2005-zhu-semi-supervised</a>
<ul>
<li>classical methods of semi-supervised learning</li>
</ul></li>
<li><a href="Learning/2017-li-noisy-labels-distillation.pdf">2017-li-noisy-labels-distillation</a>
<ul>
<li>uses distillation for dealing with noisy lables</li>
</ul></li>
<li><a href="Learning/2018-oliver-evaluation-semi-supervised.pdf">2018-oliver-evaluation-semi-supervised</a></li>
<li><a href="Learning/2018-ren-metalearning-semi-supervised.pdf">2018-ren-metalearning-semi-supervised</a></li>
<li><a href="Learning/2018-tanaka-joint-optimization-noisy-labels.pdf">2018-tanaka-joint-optimization-noisy-labels</a></li>
</ul>
<p>Examples of Unsupervised Learning:</p>
<ul>
<li><a href="Learning/2016-lin-unsupervised-binary-descriptors.pdf">2016-lin-unsupervised-binary-descriptors</a></li>
<li><a href="Learning/2016-radford-unsupervised-representation-learning.pdf">2016-radford-unsupervised-representation-learning</a></li>
<li><a href="Learning/2016-xie-unsupervised-deep-embedding.pdf">2016-xie-unsupervised-deep-embedding</a></li>
<li><a href="Learning/2017-ren-unsupervised-deep-flow.pdf">2017-ren-unsupervised-deep-flow</a></li>
<li><a href="Learning/2017-lotter-unsupervised-predictive-video-coding.pdf">2017-lotter-unsupervised-predictive-video-coding</a></li>
<li><a href="Learning/2018-li-unsupervised-odometry.pdf">2018-li-unsupervised-odometry</a></li>
</ul>
<p>Transfer and Multitask Learning:</p>
<ul>
<li><a href="Learning/2016-rusu-progressive-networks.pdf">2016-rusu-progressive-networks</a></li>
<li><p><a href="Learning/2017-ruder-multitask-survey.pdf">2017-ruder-multitask-survey</a></p></li>
<li><p><a href="Learning/2016-geng-transfer-learning-reid.pdf">2016-geng-transfer-learning-reid</a></p></li>
</ul>
<h2 id="gans">GANs</h2>
<ul>
<li><a href="General/2014-goodfellow-gans.pdf">2014-goodfellow-gans</a></li>
<li><a href="General/2015-radford-dcgan.pdf">2015-radford-dcgan</a></li>
<li><a href="General/2016-isola-image2image-gan.pdf">2016-isola-image2image-gan</a></li>
<li><a href="General/2016-salimans-improved-gan-training.pdf">2016-salimans-improved-gan-training</a></li>
</ul>
<!-- - [2016-ho-gan-imitation-learning](General/2016-ho-gan-imitation-learning.pdf) -->
<!-- # Siamese -->
<h2 id="computational-issues">Computational Issues</h2>
<ul>
<li><a href="General/2017-chen-distributed-sgd.pdf">2017-chen-distributed-sgd</a>
<ul>
<li>scaling up with many GPUs on many nodes</li>
</ul></li>
<li><a href="General/2016-iandola-squeezenet.pdf">2016-iandola-squeezenet</a>
<ul>
<li>reducing the number of parameters</li>
</ul></li>
<li><a href="General/2017-yuan-adversarial.pdf">2017-yuan-adversarial</a>
<ul>
<li>the existence and problems with adversarial samples</li>
</ul></li>
</ul>
<h1 id="surveys">Surveys</h1>
<ul>
<li><a href="General/2014-schmidhuber-deep-learning-survey.pdf">2014-schmidhuber-deep-learning-survey</a></li>
<li><a href="General/2015-lecun-nature-deep-learning.pdf">2015-lecun-nature-deep-learning</a></li>
<li><a href="OCR/2015-karpathy-recurrent-ocr.pdf">2015-karpathy-recurrent-ocr</a></li>
<li><a href="General/2018-alom-survey-imagenet.pdf">2018-alom-survey-imagenet</a></li>
</ul>
<h1 id="additional-readings">Additional Readings</h1>
<ul>
<li><a href="More/2013-goodfellow-maxout.pdf">2013-goodfellow-maxout</a></li>
<li><a href="General/2014-donahue-long-term-rcnn.pdf">2014-donahue-long-term-rcnn</a></li>
<li><a href="General/2014-karpathy-image-descriptions.pdf">2014-karpathy-image-descriptions</a></li>
<li><a href="General/2015-liu-face-attributes-wild.pdf">2015-liu-face-attributes-wild</a></li>
<li><a href="General/2015-mnih-deep-reinforcement-learning.pdf">2015-mnih-deep-reinforcement-learning</a></li>
<li><a href="General/2015-ng-video-classification.pdf">2015-ng-video-classification</a></li>
<li><a href="General/2015-yu-visual-madlibs.pdf">2015-yu-visual-madlibs</a></li>
<li><a href="General/2015-zheng-crfs-as-rnns.pdf">2015-zheng-crfs-as-rnns</a></li>
<li><a href="General/2016-abadi-tensorflow.pdf">2016-abadi-tensorflow</a></li>
<li><a href="More/2016-ba-layernorm.pdf">2016-ba-layernorm</a></li>
<li><a href="General/2016-mnih-async-dl.pdf">2016-mnih-async-dl</a></li>
<li><a href="More/2016-salimans-weightnorm.pdf">2016-salimans-weightnorm</a></li>
<li><a href="More/2016-shi-superresolution.pdf">2016-shi-superresolution</a></li>
<li><a href="More/2016-ulyanof-instancenorm.pdf">2016-ulyanof-instancenorm</a></li>
<li><a href="General/2016-vinyals-matching-networks.pdf">2016-vinyals-matching-networks</a></li>
<li><a href="General/2016-zhang-very-deep-speech.pdf">2016-zhang-very-deep-speech</a></li>
<li><a href="More/2017-barron-celu.pdf">2017-barron-celu</a></li>
<li><a href="More/2017-hochrreiter-self-normalizing-networks.pdf">2017-hochrreiter-self-normalizing-networks</a></li>
<li><a href="More/2017-ioffe-batchnorm.pdf">2017-ioffe-batchnorm</a></li>
<li><a href="General/2017-wang-tacotron.pdf">2017-wang-tacotron</a></li>
<li><a href="General/2018-burda-curiosity.pdf">2018-burda-curiosity</a></li>
<li><a href="General/2018-metz-metalearning.pdf">2018-metz-metalearning</a></li>
<li><a href="More/2018-wu-groupnorm.pdf">2018-wu-groupnorm</a></li>
</ul>
